I0318 11:57:41.935788  2460 caffe.cpp:217] Using GPUs 0
I0318 11:57:41.971325  2460 caffe.cpp:222] GPU 0: GeForce GTX TITAN X
I0318 11:57:42.314851  2460 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 5000
base_lr: 0.01
display: 1000
max_iter: 100000
lr_policy: "step"
gamma: 0.5
momentum: 0.75
weight_decay: 0.0002
stepsize: 20000
snapshot: 100000
snapshot_prefix: "mnist"
solver_mode: GPU
device_id: 0
net: "train_val_stats_2.prototxt"
train_state {
  level: 0
  stage: ""
}
test_initialization: true
average_loss: 40
I0318 11:57:42.314970  2460 solver.cpp:91] Creating training net from net file: train_val_stats_2.prototxt
I0318 11:57:42.315316  2460 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: train_val_stats_2.prototxt
I0318 11:57:42.315335  2460 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0318 11:57:42.315382  2460 net.cpp:338] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0318 11:57:42.315395  2460 net.cpp:338] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0318 11:57:42.315534  2460 net.cpp:74] Initializing net from parameters: 
name: "MNIST_NET"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  image_data_param {
    source: "train.txt"
    batch_size: 300
    crop_size: 27
    shuffle: true
    is_color: false
  }
}
layer {
  name: "data_scaling"
  type: "Power"
  bottom: "data"
  top: "data"
  power_param {
    power: 1
    scale: 0.0078125
    shift: -1
  }
}
layer {
  name: "data_drop"
  type: "Dropout"
  bottom: "data"
  top: "data"
  include {
    phase: TRAIN
  }
  dropout_param {
    dropout_ratio: 0.01
  }
}
layer {
  name: "data_vision"
  type: "VisionTransformation"
  bottom: "data"
  top: "data"
  include {
    phase: TRAIN
  }
  vision_transformation_param {
    noise_mean: 0
    noise_std: 0
    noise_std_small: 0
    rotate_min_angle: -20
    rotate_max_angle: 20
    rotate_fill_value: 0
    per_pixel_multiplier_mean: 1
    per_pixel_multiplier_std: 0
    rescale_probability: 0.25
    constant_multiplier_mean: 1
    constant_multiplier_std: 0
    scale_mean: 1
    scale_std: 0.1
    constant_multiplier_color_mean: 0
    constant_multiplier_color_std: 0
    value_cap_min: 0
    value_cap_max: 0
    passthrough_probability: 0.5
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
}
layer {
  name: "conv1_inv"
  type: "Power"
  bottom: "bn1"
  top: "conv1_inv"
  power_param {
    power: 1
    scale: -1
    shift: 0
  }
}
layer {
  name: "conv1_inv_relu"
  type: "ReLU"
  bottom: "conv1_inv"
  top: "conv1_inv"
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "bn1"
  top: "conv1_pos"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_pos"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_inv"
  type: "Convolution"
  bottom: "conv1_inv"
  top: "conv2_inv"
  param {
    name: "conv2_inv_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_inv_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "block_output"
  type: "Concat"
  bottom: "conv2"
  bottom: "conv2_inv"
  top: "block_output"
}
layer {
  name: "block_output_prelu"
  type: "PReLU"
  bottom: "block_output"
  top: "block_output"
  prelu_param {
    filler {
      type: "constant"
      value: 0.01
    }
  }
}
layer {
  name: "fc_10"
  type: "InnerProduct"
  bottom: "block_output"
  top: "fc_10"
  param {
    lr_mult: 5
    decay_mult: 1
  }
  param {
    lr_mult: 10
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc_10"
  bottom: "label"
  top: "loss"
  include {
    phase: TRAIN
  }
}
I0318 11:57:42.315635  2460 layer_factory.hpp:77] Creating layer data
I0318 11:57:42.315662  2460 net.cpp:116] Creating Layer data
I0318 11:57:42.315668  2460 net.cpp:424] data -> data
I0318 11:57:42.315685  2460 net.cpp:424] data -> label
I0318 11:57:42.315696  2460 image_data_layer.cpp:38] Opening file train.txt
I0318 11:57:42.328105  2460 image_data_layer.cpp:53] Shuffling data
I0318 11:57:42.332227  2460 image_data_layer.cpp:58] A total of 60000 images.
I0318 11:57:42.342366  2460 image_data_layer.cpp:85] output data size: 300,1,28,28
I0318 11:57:42.344805  2460 net.cpp:166] Setting up data
I0318 11:57:42.344822  2460 net.cpp:173] Top shape: 300 1 28 28 (235200)
I0318 11:57:42.344830  2460 net.cpp:173] Top shape: 300 (300)
I0318 11:57:42.344831  2460 net.cpp:181] Memory required for data: 942000
I0318 11:57:42.344838  2460 layer_factory.hpp:77] Creating layer data_scaling
I0318 11:57:42.344849  2460 net.cpp:116] Creating Layer data_scaling
I0318 11:57:42.344854  2460 net.cpp:450] data_scaling <- data
I0318 11:57:42.344862  2460 net.cpp:411] data_scaling -> data (in-place)
I0318 11:57:42.344873  2460 net.cpp:166] Setting up data_scaling
I0318 11:57:42.344877  2460 net.cpp:173] Top shape: 300 1 28 28 (235200)
I0318 11:57:42.344878  2460 net.cpp:181] Memory required for data: 1882800
I0318 11:57:42.344880  2460 layer_factory.hpp:77] Creating layer data_drop
I0318 11:57:42.344887  2460 net.cpp:116] Creating Layer data_drop
I0318 11:57:42.344889  2460 net.cpp:450] data_drop <- data
I0318 11:57:42.344892  2460 net.cpp:411] data_drop -> data (in-place)
I0318 11:57:42.344944  2460 net.cpp:166] Setting up data_drop
I0318 11:57:42.344951  2460 net.cpp:173] Top shape: 300 1 28 28 (235200)
I0318 11:57:42.344954  2460 net.cpp:181] Memory required for data: 2823600
I0318 11:57:42.344956  2460 layer_factory.hpp:77] Creating layer data_vision
I0318 11:57:42.344964  2460 net.cpp:116] Creating Layer data_vision
I0318 11:57:42.344967  2460 net.cpp:450] data_vision <- data
I0318 11:57:42.344970  2460 net.cpp:411] data_vision -> data (in-place)
I0318 11:57:42.344977  2460 net.cpp:166] Setting up data_vision
I0318 11:57:42.344980  2460 net.cpp:173] Top shape: 300 1 28 28 (235200)
I0318 11:57:42.344982  2460 net.cpp:181] Memory required for data: 3764400
I0318 11:57:42.344985  2460 layer_factory.hpp:77] Creating layer conv1
I0318 11:57:42.344997  2460 net.cpp:116] Creating Layer conv1
I0318 11:57:42.345000  2460 net.cpp:450] conv1 <- data
I0318 11:57:42.345006  2460 net.cpp:424] conv1 -> conv1
I0318 11:57:42.589920  2460 net.cpp:166] Setting up conv1
I0318 11:57:42.589956  2460 net.cpp:173] Top shape: 300 36 13 13 (1825200)
I0318 11:57:42.589959  2460 net.cpp:181] Memory required for data: 11065200
I0318 11:57:42.589980  2460 layer_factory.hpp:77] Creating layer bn1
I0318 11:57:42.589995  2460 net.cpp:116] Creating Layer bn1
I0318 11:57:42.590000  2460 net.cpp:450] bn1 <- conv1
I0318 11:57:42.590009  2460 net.cpp:424] bn1 -> bn1
I0318 11:57:42.590198  2460 net.cpp:166] Setting up bn1
I0318 11:57:42.590209  2460 net.cpp:173] Top shape: 300 36 13 13 (1825200)
I0318 11:57:42.590214  2460 net.cpp:181] Memory required for data: 18366000
I0318 11:57:42.590229  2460 layer_factory.hpp:77] Creating layer bn1_bn1_0_split
I0318 11:57:42.590239  2460 net.cpp:116] Creating Layer bn1_bn1_0_split
I0318 11:57:42.590242  2460 net.cpp:450] bn1_bn1_0_split <- bn1
I0318 11:57:42.590250  2460 net.cpp:424] bn1_bn1_0_split -> bn1_bn1_0_split_0
I0318 11:57:42.590260  2460 net.cpp:424] bn1_bn1_0_split -> bn1_bn1_0_split_1
I0318 11:57:42.590307  2460 net.cpp:166] Setting up bn1_bn1_0_split
I0318 11:57:42.590333  2460 net.cpp:173] Top shape: 300 36 13 13 (1825200)
I0318 11:57:42.590340  2460 net.cpp:173] Top shape: 300 36 13 13 (1825200)
I0318 11:57:42.590343  2460 net.cpp:181] Memory required for data: 32967600
I0318 11:57:42.590348  2460 layer_factory.hpp:77] Creating layer conv1_inv
I0318 11:57:42.590358  2460 net.cpp:116] Creating Layer conv1_inv
I0318 11:57:42.590363  2460 net.cpp:450] conv1_inv <- bn1_bn1_0_split_0
I0318 11:57:42.590369  2460 net.cpp:424] conv1_inv -> conv1_inv
I0318 11:57:42.590401  2460 net.cpp:166] Setting up conv1_inv
I0318 11:57:42.590410  2460 net.cpp:173] Top shape: 300 36 13 13 (1825200)
I0318 11:57:42.590415  2460 net.cpp:181] Memory required for data: 40268400
I0318 11:57:42.590420  2460 layer_factory.hpp:77] Creating layer conv1_inv_relu
I0318 11:57:42.590430  2460 net.cpp:116] Creating Layer conv1_inv_relu
I0318 11:57:42.590433  2460 net.cpp:450] conv1_inv_relu <- conv1_inv
I0318 11:57:42.590440  2460 net.cpp:411] conv1_inv_relu -> conv1_inv (in-place)
I0318 11:57:42.590741  2460 net.cpp:166] Setting up conv1_inv_relu
I0318 11:57:42.590755  2460 net.cpp:173] Top shape: 300 36 13 13 (1825200)
I0318 11:57:42.590759  2460 net.cpp:181] Memory required for data: 47569200
I0318 11:57:42.590764  2460 layer_factory.hpp:77] Creating layer relu1
I0318 11:57:42.590771  2460 net.cpp:116] Creating Layer relu1
I0318 11:57:42.590776  2460 net.cpp:450] relu1 <- bn1_bn1_0_split_1
I0318 11:57:42.590783  2460 net.cpp:424] relu1 -> conv1_pos
I0318 11:57:42.590947  2460 net.cpp:166] Setting up relu1
I0318 11:57:42.590970  2460 net.cpp:173] Top shape: 300 36 13 13 (1825200)
I0318 11:57:42.590975  2460 net.cpp:181] Memory required for data: 54870000
I0318 11:57:42.590981  2460 layer_factory.hpp:77] Creating layer conv2
I0318 11:57:42.590994  2460 net.cpp:116] Creating Layer conv2
I0318 11:57:42.590999  2460 net.cpp:450] conv2 <- conv1_pos
I0318 11:57:42.591008  2460 net.cpp:424] conv2 -> conv2
I0318 11:57:42.592625  2460 net.cpp:166] Setting up conv2
I0318 11:57:42.592641  2460 net.cpp:173] Top shape: 300 128 6 6 (1382400)
I0318 11:57:42.592645  2460 net.cpp:181] Memory required for data: 60399600
I0318 11:57:42.592655  2460 layer_factory.hpp:77] Creating layer conv2_inv
I0318 11:57:42.592667  2460 net.cpp:116] Creating Layer conv2_inv
I0318 11:57:42.592674  2460 net.cpp:450] conv2_inv <- conv1_inv
I0318 11:57:42.592681  2460 net.cpp:424] conv2_inv -> conv2_inv
I0318 11:57:42.593760  2460 net.cpp:166] Setting up conv2_inv
I0318 11:57:42.593775  2460 net.cpp:173] Top shape: 300 128 6 6 (1382400)
I0318 11:57:42.593780  2460 net.cpp:181] Memory required for data: 65929200
I0318 11:57:42.593791  2460 layer_factory.hpp:77] Creating layer block_output
I0318 11:57:42.593801  2460 net.cpp:116] Creating Layer block_output
I0318 11:57:42.593806  2460 net.cpp:450] block_output <- conv2
I0318 11:57:42.593812  2460 net.cpp:450] block_output <- conv2_inv
I0318 11:57:42.593818  2460 net.cpp:424] block_output -> block_output
I0318 11:57:42.593853  2460 net.cpp:166] Setting up block_output
I0318 11:57:42.593864  2460 net.cpp:173] Top shape: 300 256 6 6 (2764800)
I0318 11:57:42.593868  2460 net.cpp:181] Memory required for data: 76988400
I0318 11:57:42.593873  2460 layer_factory.hpp:77] Creating layer block_output_prelu
I0318 11:57:42.593881  2460 net.cpp:116] Creating Layer block_output_prelu
I0318 11:57:42.593886  2460 net.cpp:450] block_output_prelu <- block_output
I0318 11:57:42.593894  2460 net.cpp:411] block_output_prelu -> block_output (in-place)
I0318 11:57:42.594375  2460 net.cpp:166] Setting up block_output_prelu
I0318 11:57:42.594388  2460 net.cpp:173] Top shape: 300 256 6 6 (2764800)
I0318 11:57:42.594393  2460 net.cpp:181] Memory required for data: 88047600
I0318 11:57:42.594400  2460 layer_factory.hpp:77] Creating layer fc_10
I0318 11:57:42.594411  2460 net.cpp:116] Creating Layer fc_10
I0318 11:57:42.594416  2460 net.cpp:450] fc_10 <- block_output
I0318 11:57:42.594424  2460 net.cpp:424] fc_10 -> fc_10
I0318 11:57:42.597240  2460 net.cpp:166] Setting up fc_10
I0318 11:57:42.597254  2460 net.cpp:173] Top shape: 300 10 (3000)
I0318 11:57:42.597273  2460 net.cpp:181] Memory required for data: 88059600
I0318 11:57:42.597283  2460 layer_factory.hpp:77] Creating layer loss
I0318 11:57:42.597293  2460 net.cpp:116] Creating Layer loss
I0318 11:57:42.597298  2460 net.cpp:450] loss <- fc_10
I0318 11:57:42.597304  2460 net.cpp:450] loss <- label
I0318 11:57:42.597312  2460 net.cpp:424] loss -> loss
I0318 11:57:42.597327  2460 layer_factory.hpp:77] Creating layer loss
I0318 11:57:42.598101  2460 net.cpp:166] Setting up loss
I0318 11:57:42.598115  2460 net.cpp:173] Top shape: (1)
I0318 11:57:42.598119  2460 net.cpp:176]     with loss weight 1
I0318 11:57:42.598141  2460 net.cpp:181] Memory required for data: 88059604
I0318 11:57:42.598146  2460 net.cpp:242] loss needs backward computation.
I0318 11:57:42.598151  2460 net.cpp:242] fc_10 needs backward computation.
I0318 11:57:42.598156  2460 net.cpp:242] block_output_prelu needs backward computation.
I0318 11:57:42.598160  2460 net.cpp:242] block_output needs backward computation.
I0318 11:57:42.598165  2460 net.cpp:242] conv2_inv needs backward computation.
I0318 11:57:42.598168  2460 net.cpp:242] conv2 needs backward computation.
I0318 11:57:42.598173  2460 net.cpp:242] relu1 needs backward computation.
I0318 11:57:42.598177  2460 net.cpp:242] conv1_inv_relu needs backward computation.
I0318 11:57:42.598181  2460 net.cpp:242] conv1_inv needs backward computation.
I0318 11:57:42.598186  2460 net.cpp:242] bn1_bn1_0_split needs backward computation.
I0318 11:57:42.598191  2460 net.cpp:242] bn1 needs backward computation.
I0318 11:57:42.598196  2460 net.cpp:242] conv1 needs backward computation.
I0318 11:57:42.598199  2460 net.cpp:244] data_vision does not need backward computation.
I0318 11:57:42.598203  2460 net.cpp:244] data_drop does not need backward computation.
I0318 11:57:42.598207  2460 net.cpp:244] data_scaling does not need backward computation.
I0318 11:57:42.598212  2460 net.cpp:244] data does not need backward computation.
I0318 11:57:42.598215  2460 net.cpp:286] This network produces output loss
I0318 11:57:42.598232  2460 net.cpp:299] Network initialization done.
I0318 11:57:42.598633  2460 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: train_val_stats_2.prototxt
I0318 11:57:42.598641  2460 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0318 11:57:42.598649  2460 solver.cpp:181] Creating test net (#0) specified by net file: train_val_stats_2.prototxt
I0318 11:57:42.598685  2460 net.cpp:338] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0318 11:57:42.598695  2460 net.cpp:338] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_drop
I0318 11:57:42.598701  2460 net.cpp:338] The NetState phase (1) differed from the phase (0) specified by a rule in layer data_vision
I0318 11:57:42.598713  2460 net.cpp:338] The NetState phase (1) differed from the phase (0) specified by a rule in layer loss
I0318 11:57:42.598844  2460 net.cpp:74] Initializing net from parameters: 
name: "MNIST_NET"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  image_data_param {
    source: "test.txt"
    batch_size: 100
    crop_size: 27
    shuffle: true
    is_color: false
  }
}
layer {
  name: "data_scaling"
  type: "Power"
  bottom: "data"
  top: "data"
  power_param {
    power: 1
    scale: 0.0078125
    shift: -1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 36
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
}
layer {
  name: "conv1_inv"
  type: "Power"
  bottom: "bn1"
  top: "conv1_inv"
  power_param {
    power: 1
    scale: -1
    shift: 0
  }
}
layer {
  name: "conv1_inv_relu"
  type: "ReLU"
  bottom: "conv1_inv"
  top: "conv1_inv"
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "bn1"
  top: "conv1_pos"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "conv1_pos"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "conv2_inv"
  type: "Convolution"
  bottom: "conv1_inv"
  top: "conv2_inv"
  param {
    name: "conv2_inv_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_inv_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "block_output"
  type: "Concat"
  bottom: "conv2"
  bottom: "conv2_inv"
  top: "block_output"
}
layer {
  name: "block_output_prelu"
  type: "PReLU"
  bottom: "block_output"
  top: "block_output"
  prelu_param {
    filler {
      type: "constant"
      value: 0.01
    }
  }
}
layer {
  name: "fc_10"
  type: "InnerProduct"
  bottom: "block_output"
  top: "fc_10"
  param {
    lr_mult: 5
    decay_mult: 1
  }
  param {
    lr_mult: 10
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc_10"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
I0318 11:57:42.598939  2460 layer_factory.hpp:77] Creating layer data
I0318 11:57:42.598964  2460 net.cpp:116] Creating Layer data
I0318 11:57:42.598973  2460 net.cpp:424] data -> data
I0318 11:57:42.598984  2460 net.cpp:424] data -> label
I0318 11:57:42.598994  2460 image_data_layer.cpp:38] Opening file test.txt
I0318 11:57:42.601395  2460 image_data_layer.cpp:53] Shuffling data
I0318 11:57:42.601994  2460 image_data_layer.cpp:58] A total of 10000 images.
I0318 11:57:42.602136  2460 image_data_layer.cpp:85] output data size: 100,1,28,28
I0318 11:57:42.603021  2460 net.cpp:166] Setting up data
I0318 11:57:42.603035  2460 net.cpp:173] Top shape: 100 1 28 28 (78400)
I0318 11:57:42.603041  2460 net.cpp:173] Top shape: 100 (100)
I0318 11:57:42.603045  2460 net.cpp:181] Memory required for data: 314000
I0318 11:57:42.603050  2460 layer_factory.hpp:77] Creating layer data_scaling
I0318 11:57:42.603060  2460 net.cpp:116] Creating Layer data_scaling
I0318 11:57:42.603065  2460 net.cpp:450] data_scaling <- data
I0318 11:57:42.603071  2460 net.cpp:411] data_scaling -> data (in-place)
I0318 11:57:42.603080  2460 net.cpp:166] Setting up data_scaling
I0318 11:57:42.603087  2460 net.cpp:173] Top shape: 100 1 28 28 (78400)
I0318 11:57:42.603091  2460 net.cpp:181] Memory required for data: 627600
I0318 11:57:42.603094  2460 layer_factory.hpp:77] Creating layer conv1
I0318 11:57:42.603106  2460 net.cpp:116] Creating Layer conv1
I0318 11:57:42.603111  2460 net.cpp:450] conv1 <- data
I0318 11:57:42.603117  2460 net.cpp:424] conv1 -> conv1
I0318 11:57:42.604074  2460 net.cpp:166] Setting up conv1
I0318 11:57:42.604089  2460 net.cpp:173] Top shape: 100 36 13 13 (608400)
I0318 11:57:42.604094  2460 net.cpp:181] Memory required for data: 3061200
I0318 11:57:42.604105  2460 layer_factory.hpp:77] Creating layer bn1
I0318 11:57:42.604115  2460 net.cpp:116] Creating Layer bn1
I0318 11:57:42.604120  2460 net.cpp:450] bn1 <- conv1
I0318 11:57:42.604127  2460 net.cpp:424] bn1 -> bn1
I0318 11:57:42.604317  2460 net.cpp:166] Setting up bn1
I0318 11:57:42.604326  2460 net.cpp:173] Top shape: 100 36 13 13 (608400)
I0318 11:57:42.604331  2460 net.cpp:181] Memory required for data: 5494800
I0318 11:57:42.604344  2460 layer_factory.hpp:77] Creating layer bn1_bn1_0_split
I0318 11:57:42.604353  2460 net.cpp:116] Creating Layer bn1_bn1_0_split
I0318 11:57:42.604357  2460 net.cpp:450] bn1_bn1_0_split <- bn1
I0318 11:57:42.604363  2460 net.cpp:424] bn1_bn1_0_split -> bn1_bn1_0_split_0
I0318 11:57:42.604385  2460 net.cpp:424] bn1_bn1_0_split -> bn1_bn1_0_split_1
I0318 11:57:42.604432  2460 net.cpp:166] Setting up bn1_bn1_0_split
I0318 11:57:42.604442  2460 net.cpp:173] Top shape: 100 36 13 13 (608400)
I0318 11:57:42.604449  2460 net.cpp:173] Top shape: 100 36 13 13 (608400)
I0318 11:57:42.604451  2460 net.cpp:181] Memory required for data: 10362000
I0318 11:57:42.604455  2460 layer_factory.hpp:77] Creating layer conv1_inv
I0318 11:57:42.604463  2460 net.cpp:116] Creating Layer conv1_inv
I0318 11:57:42.604467  2460 net.cpp:450] conv1_inv <- bn1_bn1_0_split_0
I0318 11:57:42.604475  2460 net.cpp:424] conv1_inv -> conv1_inv
I0318 11:57:42.604517  2460 net.cpp:166] Setting up conv1_inv
I0318 11:57:42.604526  2460 net.cpp:173] Top shape: 100 36 13 13 (608400)
I0318 11:57:42.604529  2460 net.cpp:181] Memory required for data: 12795600
I0318 11:57:42.604534  2460 layer_factory.hpp:77] Creating layer conv1_inv_relu
I0318 11:57:42.604540  2460 net.cpp:116] Creating Layer conv1_inv_relu
I0318 11:57:42.604545  2460 net.cpp:450] conv1_inv_relu <- conv1_inv
I0318 11:57:42.604552  2460 net.cpp:411] conv1_inv_relu -> conv1_inv (in-place)
I0318 11:57:42.604851  2460 net.cpp:166] Setting up conv1_inv_relu
I0318 11:57:42.604864  2460 net.cpp:173] Top shape: 100 36 13 13 (608400)
I0318 11:57:42.604868  2460 net.cpp:181] Memory required for data: 15229200
I0318 11:57:42.604873  2460 layer_factory.hpp:77] Creating layer relu1
I0318 11:57:42.604882  2460 net.cpp:116] Creating Layer relu1
I0318 11:57:42.604885  2460 net.cpp:450] relu1 <- bn1_bn1_0_split_1
I0318 11:57:42.604892  2460 net.cpp:424] relu1 -> conv1_pos
I0318 11:57:42.605060  2460 net.cpp:166] Setting up relu1
I0318 11:57:42.605073  2460 net.cpp:173] Top shape: 100 36 13 13 (608400)
I0318 11:57:42.605077  2460 net.cpp:181] Memory required for data: 17662800
I0318 11:57:42.605082  2460 layer_factory.hpp:77] Creating layer conv2
I0318 11:57:42.605094  2460 net.cpp:116] Creating Layer conv2
I0318 11:57:42.605099  2460 net.cpp:450] conv2 <- conv1_pos
I0318 11:57:42.605108  2460 net.cpp:424] conv2 -> conv2
I0318 11:57:42.606181  2460 net.cpp:166] Setting up conv2
I0318 11:57:42.606197  2460 net.cpp:173] Top shape: 100 128 6 6 (460800)
I0318 11:57:42.606201  2460 net.cpp:181] Memory required for data: 19506000
I0318 11:57:42.606209  2460 layer_factory.hpp:77] Creating layer conv2_inv
I0318 11:57:42.606220  2460 net.cpp:116] Creating Layer conv2_inv
I0318 11:57:42.606227  2460 net.cpp:450] conv2_inv <- conv1_inv
I0318 11:57:42.606235  2460 net.cpp:424] conv2_inv -> conv2_inv
I0318 11:57:42.607321  2460 net.cpp:166] Setting up conv2_inv
I0318 11:57:42.607336  2460 net.cpp:173] Top shape: 100 128 6 6 (460800)
I0318 11:57:42.607341  2460 net.cpp:181] Memory required for data: 21349200
I0318 11:57:42.607352  2460 layer_factory.hpp:77] Creating layer block_output
I0318 11:57:42.607360  2460 net.cpp:116] Creating Layer block_output
I0318 11:57:42.607365  2460 net.cpp:450] block_output <- conv2
I0318 11:57:42.607372  2460 net.cpp:450] block_output <- conv2_inv
I0318 11:57:42.607378  2460 net.cpp:424] block_output -> block_output
I0318 11:57:42.607414  2460 net.cpp:166] Setting up block_output
I0318 11:57:42.607425  2460 net.cpp:173] Top shape: 100 256 6 6 (921600)
I0318 11:57:42.607429  2460 net.cpp:181] Memory required for data: 25035600
I0318 11:57:42.607434  2460 layer_factory.hpp:77] Creating layer block_output_prelu
I0318 11:57:42.607451  2460 net.cpp:116] Creating Layer block_output_prelu
I0318 11:57:42.607455  2460 net.cpp:450] block_output_prelu <- block_output
I0318 11:57:42.607462  2460 net.cpp:411] block_output_prelu -> block_output (in-place)
I0318 11:57:42.607550  2460 net.cpp:166] Setting up block_output_prelu
I0318 11:57:42.607559  2460 net.cpp:173] Top shape: 100 256 6 6 (921600)
I0318 11:57:42.607563  2460 net.cpp:181] Memory required for data: 28722000
I0318 11:57:42.607570  2460 layer_factory.hpp:77] Creating layer fc_10
I0318 11:57:42.607579  2460 net.cpp:116] Creating Layer fc_10
I0318 11:57:42.607584  2460 net.cpp:450] fc_10 <- block_output
I0318 11:57:42.607605  2460 net.cpp:424] fc_10 -> fc_10
I0318 11:57:42.609900  2460 net.cpp:166] Setting up fc_10
I0318 11:57:42.609912  2460 net.cpp:173] Top shape: 100 10 (1000)
I0318 11:57:42.609916  2460 net.cpp:181] Memory required for data: 28726000
I0318 11:57:42.609925  2460 layer_factory.hpp:77] Creating layer accuracy
I0318 11:57:42.609932  2460 net.cpp:116] Creating Layer accuracy
I0318 11:57:42.609937  2460 net.cpp:450] accuracy <- fc_10
I0318 11:57:42.609943  2460 net.cpp:450] accuracy <- label
I0318 11:57:42.609949  2460 net.cpp:424] accuracy -> accuracy
I0318 11:57:42.609961  2460 net.cpp:166] Setting up accuracy
I0318 11:57:42.609967  2460 net.cpp:173] Top shape: (1)
I0318 11:57:42.609971  2460 net.cpp:181] Memory required for data: 28726004
I0318 11:57:42.609977  2460 net.cpp:244] accuracy does not need backward computation.
I0318 11:57:42.609980  2460 net.cpp:244] fc_10 does not need backward computation.
I0318 11:57:42.609984  2460 net.cpp:244] block_output_prelu does not need backward computation.
I0318 11:57:42.609988  2460 net.cpp:244] block_output does not need backward computation.
I0318 11:57:42.609992  2460 net.cpp:244] conv2_inv does not need backward computation.
I0318 11:57:42.609997  2460 net.cpp:244] conv2 does not need backward computation.
I0318 11:57:42.610000  2460 net.cpp:244] relu1 does not need backward computation.
I0318 11:57:42.610004  2460 net.cpp:244] conv1_inv_relu does not need backward computation.
I0318 11:57:42.610008  2460 net.cpp:244] conv1_inv does not need backward computation.
I0318 11:57:42.610013  2460 net.cpp:244] bn1_bn1_0_split does not need backward computation.
I0318 11:57:42.610016  2460 net.cpp:244] bn1 does not need backward computation.
I0318 11:57:42.610020  2460 net.cpp:244] conv1 does not need backward computation.
I0318 11:57:42.610025  2460 net.cpp:244] data_scaling does not need backward computation.
I0318 11:57:42.610029  2460 net.cpp:244] data does not need backward computation.
I0318 11:57:42.610033  2460 net.cpp:286] This network produces output accuracy
I0318 11:57:42.610046  2460 net.cpp:299] Network initialization done.
I0318 11:57:42.610102  2460 solver.cpp:60] Solver scaffolding done.
I0318 11:57:42.610462  2460 caffe.cpp:251] Starting Optimization
I0318 11:57:42.610472  2460 solver.cpp:279] Solving MNIST_NET
I0318 11:57:42.610476  2460 solver.cpp:280] Learning Rate Policy: step
I0318 11:57:42.610981  2460 solver.cpp:337] Iteration 0, Testing net (#0)
I0318 11:57:42.610993  2460 net.cpp:709] Ignoring source layer data_drop
I0318 11:57:42.610996  2460 net.cpp:709] Ignoring source layer data_vision
I0318 11:57:42.611129  2460 net.cpp:709] Ignoring source layer loss
I0318 11:57:42.615191  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:57:42.914837  2460 solver.cpp:404]     Test net output #0: accuracy = 0.0651
I0318 11:57:42.939524  2460 solver.cpp:228] Iteration 0, loss = 2.525
I0318 11:57:42.939550  2460 solver.cpp:244]     Train net output #0: loss = 2.525 (* 1 = 2.525 loss)
I0318 11:57:42.939559  2460 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0318 11:57:51.919817  2460 solver.cpp:228] Iteration 1000, loss = 0.0939186
I0318 11:57:51.919845  2460 solver.cpp:244]     Train net output #0: loss = 0.0648256 (* 1 = 0.0648256 loss)
I0318 11:57:51.919850  2460 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0318 11:57:55.520086  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:58:01.881135  2460 solver.cpp:228] Iteration 2000, loss = 0.0645458
I0318 11:58:01.881162  2460 solver.cpp:244]     Train net output #0: loss = 0.0453376 (* 1 = 0.0453376 loss)
I0318 11:58:01.881166  2460 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I0318 11:58:07.909962  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:58:10.913336  2460 solver.cpp:228] Iteration 3000, loss = 0.0557694
I0318 11:58:10.913365  2460 solver.cpp:244]     Train net output #0: loss = 0.0515671 (* 1 = 0.0515671 loss)
I0318 11:58:10.913370  2460 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I0318 11:58:19.970981  2460 solver.cpp:228] Iteration 4000, loss = 0.052101
I0318 11:58:19.971086  2460 solver.cpp:244]     Train net output #0: loss = 0.0559698 (* 1 = 0.0559698 loss)
I0318 11:58:19.971093  2460 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I0318 11:58:20.546123  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:58:29.064401  2460 solver.cpp:337] Iteration 5000, Testing net (#0)
I0318 11:58:29.064417  2460 net.cpp:709] Ignoring source layer data_drop
I0318 11:58:29.064421  2460 net.cpp:709] Ignoring source layer data_vision
I0318 11:58:29.064427  2460 net.cpp:709] Ignoring source layer loss
I0318 11:58:29.295658  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9892
I0318 11:58:29.304126  2460 solver.cpp:228] Iteration 5000, loss = 0.0461117
I0318 11:58:29.304162  2460 solver.cpp:244]     Train net output #0: loss = 0.0352601 (* 1 = 0.0352601 loss)
I0318 11:58:29.304172  2460 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I0318 11:58:36.510262  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:58:38.360975  2460 solver.cpp:228] Iteration 6000, loss = 0.0350638
I0318 11:58:38.361002  2460 solver.cpp:244]     Train net output #0: loss = 0.0438662 (* 1 = 0.0438662 loss)
I0318 11:58:38.361006  2460 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I0318 11:58:47.585274  2460 solver.cpp:228] Iteration 7000, loss = 0.0415482
I0318 11:58:47.585302  2460 solver.cpp:244]     Train net output #0: loss = 0.0586841 (* 1 = 0.0586841 loss)
I0318 11:58:47.585307  2460 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I0318 11:58:49.854135  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:58:56.763947  2460 solver.cpp:228] Iteration 8000, loss = 0.0358212
I0318 11:58:56.763999  2460 solver.cpp:244]     Train net output #0: loss = 0.0718116 (* 1 = 0.0718116 loss)
I0318 11:58:56.764004  2460 sgd_solver.cpp:106] Iteration 8000, lr = 0.01
I0318 11:59:03.491856  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:59:05.899518  2460 solver.cpp:228] Iteration 9000, loss = 0.0370316
I0318 11:59:05.899547  2460 solver.cpp:244]     Train net output #0: loss = 0.0121591 (* 1 = 0.0121591 loss)
I0318 11:59:05.899552  2460 sgd_solver.cpp:106] Iteration 9000, lr = 0.01
I0318 11:59:14.965917  2460 solver.cpp:337] Iteration 10000, Testing net (#0)
I0318 11:59:14.965939  2460 net.cpp:709] Ignoring source layer data_drop
I0318 11:59:14.965941  2460 net.cpp:709] Ignoring source layer data_vision
I0318 11:59:14.965945  2460 net.cpp:709] Ignoring source layer loss
I0318 11:59:15.131098  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:59:15.263497  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9907
I0318 11:59:15.268924  2460 solver.cpp:228] Iteration 10000, loss = 0.036111
I0318 11:59:15.268942  2460 solver.cpp:244]     Train net output #0: loss = 0.0287326 (* 1 = 0.0287326 loss)
I0318 11:59:15.268949  2460 sgd_solver.cpp:106] Iteration 10000, lr = 0.01
I0318 11:59:24.591409  2460 solver.cpp:228] Iteration 11000, loss = 0.0339102
I0318 11:59:24.591437  2460 solver.cpp:244]     Train net output #0: loss = 0.0331509 (* 1 = 0.0331509 loss)
I0318 11:59:24.591442  2460 sgd_solver.cpp:106] Iteration 11000, lr = 0.01
I0318 11:59:28.272855  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:59:33.699753  2460 solver.cpp:228] Iteration 12000, loss = 0.0320286
I0318 11:59:33.699781  2460 solver.cpp:244]     Train net output #0: loss = 0.0214517 (* 1 = 0.0214517 loss)
I0318 11:59:33.699786  2460 sgd_solver.cpp:106] Iteration 12000, lr = 0.01
I0318 11:59:41.879422  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 11:59:42.829582  2460 solver.cpp:228] Iteration 13000, loss = 0.0302935
I0318 11:59:42.829612  2460 solver.cpp:244]     Train net output #0: loss = 0.0140985 (* 1 = 0.0140985 loss)
I0318 11:59:42.829615  2460 sgd_solver.cpp:106] Iteration 13000, lr = 0.01
I0318 11:59:52.022078  2460 solver.cpp:228] Iteration 14000, loss = 0.025443
I0318 11:59:52.022109  2460 solver.cpp:244]     Train net output #0: loss = 0.0514578 (* 1 = 0.0514578 loss)
I0318 11:59:52.022114  2460 sgd_solver.cpp:106] Iteration 14000, lr = 0.01
I0318 11:59:54.702376  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:00:01.249935  2460 solver.cpp:337] Iteration 15000, Testing net (#0)
I0318 12:00:01.250018  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:00:01.250022  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:00:01.250027  2460 net.cpp:709] Ignoring source layer loss
I0318 12:00:01.483098  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9903
I0318 12:00:01.490046  2460 solver.cpp:228] Iteration 15000, loss = 0.0267188
I0318 12:00:01.490064  2460 solver.cpp:244]     Train net output #0: loss = 0.00832219 (* 1 = 0.00832219 loss)
I0318 12:00:01.490069  2460 sgd_solver.cpp:106] Iteration 15000, lr = 0.01
I0318 12:00:05.826140  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:00:10.681427  2460 solver.cpp:228] Iteration 16000, loss = 0.0305184
I0318 12:00:10.681454  2460 solver.cpp:244]     Train net output #0: loss = 0.0371357 (* 1 = 0.0371357 loss)
I0318 12:00:10.681459  2460 sgd_solver.cpp:106] Iteration 16000, lr = 0.01
I0318 12:00:17.889078  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:00:19.934401  2460 solver.cpp:228] Iteration 17000, loss = 0.0282579
I0318 12:00:19.934427  2460 solver.cpp:244]     Train net output #0: loss = 0.0258644 (* 1 = 0.0258644 loss)
I0318 12:00:19.934432  2460 sgd_solver.cpp:106] Iteration 17000, lr = 0.01
I0318 12:00:29.077951  2460 solver.cpp:228] Iteration 18000, loss = 0.0255527
I0318 12:00:29.077980  2460 solver.cpp:244]     Train net output #0: loss = 0.016478 (* 1 = 0.016478 loss)
I0318 12:00:29.077985  2460 sgd_solver.cpp:106] Iteration 18000, lr = 0.01
I0318 12:00:33.472146  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:00:38.287130  2460 solver.cpp:228] Iteration 19000, loss = 0.0260679
I0318 12:00:38.287165  2460 solver.cpp:244]     Train net output #0: loss = 0.0109281 (* 1 = 0.0109281 loss)
I0318 12:00:38.287173  2460 sgd_solver.cpp:106] Iteration 19000, lr = 0.01
I0318 12:00:46.875833  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:00:47.331687  2460 solver.cpp:337] Iteration 20000, Testing net (#0)
I0318 12:00:47.331702  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:00:47.331706  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:00:47.331709  2460 net.cpp:709] Ignoring source layer loss
I0318 12:00:47.564211  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9922
I0318 12:00:47.571252  2460 solver.cpp:228] Iteration 20000, loss = 0.0251695
I0318 12:00:47.571271  2460 solver.cpp:244]     Train net output #0: loss = 0.0211965 (* 1 = 0.0211965 loss)
I0318 12:00:47.571277  2460 sgd_solver.cpp:106] Iteration 20000, lr = 0.005
I0318 12:00:56.774507  2460 solver.cpp:228] Iteration 21000, loss = 0.020931
I0318 12:00:56.774543  2460 solver.cpp:244]     Train net output #0: loss = 0.0142012 (* 1 = 0.0142012 loss)
I0318 12:00:56.774550  2460 sgd_solver.cpp:106] Iteration 21000, lr = 0.005
I0318 12:00:59.537930  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:01:06.031849  2460 solver.cpp:228] Iteration 22000, loss = 0.0248528
I0318 12:01:06.031910  2460 solver.cpp:244]     Train net output #0: loss = 0.0402345 (* 1 = 0.0402345 loss)
I0318 12:01:06.031916  2460 sgd_solver.cpp:106] Iteration 22000, lr = 0.005
I0318 12:01:12.138146  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:01:15.014155  2460 solver.cpp:228] Iteration 23000, loss = 0.0241324
I0318 12:01:15.014183  2460 solver.cpp:244]     Train net output #0: loss = 0.0253294 (* 1 = 0.0253294 loss)
I0318 12:01:15.014188  2460 sgd_solver.cpp:106] Iteration 23000, lr = 0.005
I0318 12:01:23.954255  2460 solver.cpp:228] Iteration 24000, loss = 0.0223679
I0318 12:01:23.954285  2460 solver.cpp:244]     Train net output #0: loss = 0.0294236 (* 1 = 0.0294236 loss)
I0318 12:01:23.954289  2460 sgd_solver.cpp:106] Iteration 24000, lr = 0.005
I0318 12:01:24.947721  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:01:33.106535  2460 solver.cpp:337] Iteration 25000, Testing net (#0)
I0318 12:01:33.106551  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:01:33.106554  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:01:33.106559  2460 net.cpp:709] Ignoring source layer loss
I0318 12:01:33.350052  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9917
I0318 12:01:33.356953  2460 solver.cpp:228] Iteration 25000, loss = 0.0211512
I0318 12:01:33.356973  2460 solver.cpp:244]     Train net output #0: loss = 0.0212132 (* 1 = 0.0212132 loss)
I0318 12:01:33.356979  2460 sgd_solver.cpp:106] Iteration 25000, lr = 0.005
I0318 12:01:40.514087  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:01:42.532949  2460 solver.cpp:228] Iteration 26000, loss = 0.0193642
I0318 12:01:42.532977  2460 solver.cpp:244]     Train net output #0: loss = 0.0121297 (* 1 = 0.0121297 loss)
I0318 12:01:42.532982  2460 sgd_solver.cpp:106] Iteration 26000, lr = 0.005
I0318 12:01:51.611819  2460 solver.cpp:228] Iteration 27000, loss = 0.0261927
I0318 12:01:51.611845  2460 solver.cpp:244]     Train net output #0: loss = 0.0207712 (* 1 = 0.0207712 loss)
I0318 12:01:51.611850  2460 sgd_solver.cpp:106] Iteration 27000, lr = 0.005
I0318 12:01:54.409914  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:02:00.599349  2460 solver.cpp:228] Iteration 28000, loss = 0.0259359
I0318 12:02:00.599377  2460 solver.cpp:244]     Train net output #0: loss = 0.0142099 (* 1 = 0.0142099 loss)
I0318 12:02:00.599381  2460 sgd_solver.cpp:106] Iteration 28000, lr = 0.005
I0318 12:02:08.122982  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:02:09.648720  2460 solver.cpp:228] Iteration 29000, loss = 0.0215006
I0318 12:02:09.648748  2460 solver.cpp:244]     Train net output #0: loss = 0.0541185 (* 1 = 0.0541185 loss)
I0318 12:02:09.648752  2460 sgd_solver.cpp:106] Iteration 29000, lr = 0.005
I0318 12:02:18.654093  2460 solver.cpp:337] Iteration 30000, Testing net (#0)
I0318 12:02:18.654157  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:02:18.654160  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:02:18.654165  2460 net.cpp:709] Ignoring source layer loss
I0318 12:02:18.886427  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9924
I0318 12:02:18.893051  2460 solver.cpp:228] Iteration 30000, loss = 0.0180923
I0318 12:02:18.893072  2460 solver.cpp:244]     Train net output #0: loss = 0.0139336 (* 1 = 0.0139336 loss)
I0318 12:02:18.893080  2460 sgd_solver.cpp:106] Iteration 30000, lr = 0.005
I0318 12:02:21.787714  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:02:28.160531  2460 solver.cpp:228] Iteration 31000, loss = 0.0222648
I0318 12:02:28.160560  2460 solver.cpp:244]     Train net output #0: loss = 0.0395586 (* 1 = 0.0395586 loss)
I0318 12:02:28.160567  2460 sgd_solver.cpp:106] Iteration 31000, lr = 0.005
I0318 12:02:34.687011  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:02:37.430413  2460 solver.cpp:228] Iteration 32000, loss = 0.017524
I0318 12:02:37.430447  2460 solver.cpp:244]     Train net output #0: loss = 0.0111624 (* 1 = 0.0111624 loss)
I0318 12:02:37.430454  2460 sgd_solver.cpp:106] Iteration 32000, lr = 0.005
I0318 12:02:46.558132  2460 solver.cpp:228] Iteration 33000, loss = 0.0232909
I0318 12:02:46.558161  2460 solver.cpp:244]     Train net output #0: loss = 0.0168909 (* 1 = 0.0168909 loss)
I0318 12:02:46.558166  2460 sgd_solver.cpp:106] Iteration 33000, lr = 0.005
I0318 12:02:49.258126  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:02:55.730337  2460 solver.cpp:228] Iteration 34000, loss = 0.0196051
I0318 12:02:55.730366  2460 solver.cpp:244]     Train net output #0: loss = 0.0194029 (* 1 = 0.0194029 loss)
I0318 12:02:55.730371  2460 sgd_solver.cpp:106] Iteration 34000, lr = 0.005
I0318 12:03:02.893733  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:03:04.990792  2460 solver.cpp:337] Iteration 35000, Testing net (#0)
I0318 12:03:04.990808  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:03:04.990810  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:03:04.990814  2460 net.cpp:709] Ignoring source layer loss
I0318 12:03:05.222707  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9917
I0318 12:03:05.230301  2460 solver.cpp:228] Iteration 35000, loss = 0.0157014
I0318 12:03:05.230320  2460 solver.cpp:244]     Train net output #0: loss = 0.00691735 (* 1 = 0.00691735 loss)
I0318 12:03:05.230326  2460 sgd_solver.cpp:106] Iteration 35000, lr = 0.005
I0318 12:03:14.464339  2460 solver.cpp:228] Iteration 36000, loss = 0.0202815
I0318 12:03:14.464366  2460 solver.cpp:244]     Train net output #0: loss = 0.016503 (* 1 = 0.016503 loss)
I0318 12:03:14.464371  2460 sgd_solver.cpp:106] Iteration 36000, lr = 0.005
I0318 12:03:16.185863  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:03:23.580663  2460 solver.cpp:228] Iteration 37000, loss = 0.0229463
I0318 12:03:23.580775  2460 solver.cpp:244]     Train net output #0: loss = 0.015206 (* 1 = 0.015206 loss)
I0318 12:03:23.580780  2460 sgd_solver.cpp:106] Iteration 37000, lr = 0.005
I0318 12:03:31.211163  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:03:32.792331  2460 solver.cpp:228] Iteration 38000, loss = 0.0181498
I0318 12:03:32.792361  2460 solver.cpp:244]     Train net output #0: loss = 0.0107047 (* 1 = 0.0107047 loss)
I0318 12:03:32.792366  2460 sgd_solver.cpp:106] Iteration 38000, lr = 0.005
I0318 12:03:41.845690  2460 solver.cpp:228] Iteration 39000, loss = 0.0192176
I0318 12:03:41.845717  2460 solver.cpp:244]     Train net output #0: loss = 0.0128331 (* 1 = 0.0128331 loss)
I0318 12:03:41.845722  2460 sgd_solver.cpp:106] Iteration 39000, lr = 0.005
I0318 12:03:45.566154  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:03:51.075119  2460 solver.cpp:337] Iteration 40000, Testing net (#0)
I0318 12:03:51.075136  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:03:51.075139  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:03:51.075143  2460 net.cpp:709] Ignoring source layer loss
I0318 12:03:51.307417  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9925
I0318 12:03:51.312886  2460 solver.cpp:228] Iteration 40000, loss = 0.0197728
I0318 12:03:51.312904  2460 solver.cpp:244]     Train net output #0: loss = 0.0142098 (* 1 = 0.0142098 loss)
I0318 12:03:51.312911  2460 sgd_solver.cpp:106] Iteration 40000, lr = 0.0025
I0318 12:03:59.628198  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:04:00.518343  2460 solver.cpp:228] Iteration 41000, loss = 0.0166359
I0318 12:04:00.518373  2460 solver.cpp:244]     Train net output #0: loss = 0.0107694 (* 1 = 0.0107694 loss)
I0318 12:04:00.518378  2460 sgd_solver.cpp:106] Iteration 41000, lr = 0.0025
I0318 12:04:09.789264  2460 solver.cpp:228] Iteration 42000, loss = 0.0185046
I0318 12:04:09.789293  2460 solver.cpp:244]     Train net output #0: loss = 0.0129981 (* 1 = 0.0129981 loss)
I0318 12:04:09.789296  2460 sgd_solver.cpp:106] Iteration 42000, lr = 0.0025
I0318 12:04:13.266705  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:04:19.178234  2460 solver.cpp:228] Iteration 43000, loss = 0.0198114
I0318 12:04:19.178262  2460 solver.cpp:244]     Train net output #0: loss = 0.0308412 (* 1 = 0.0308412 loss)
I0318 12:04:19.178267  2460 sgd_solver.cpp:106] Iteration 43000, lr = 0.0025
I0318 12:04:26.422137  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:04:28.398327  2460 solver.cpp:228] Iteration 44000, loss = 0.018749
I0318 12:04:28.398356  2460 solver.cpp:244]     Train net output #0: loss = 0.00972463 (* 1 = 0.00972463 loss)
I0318 12:04:28.398360  2460 sgd_solver.cpp:106] Iteration 44000, lr = 0.0025
I0318 12:04:37.643762  2460 solver.cpp:337] Iteration 45000, Testing net (#0)
I0318 12:04:37.643822  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:04:37.643826  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:04:37.643831  2460 net.cpp:709] Ignoring source layer loss
I0318 12:04:37.874766  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9922
I0318 12:04:37.880308  2460 solver.cpp:228] Iteration 45000, loss = 0.0183845
I0318 12:04:37.880327  2460 solver.cpp:244]     Train net output #0: loss = 0.0277894 (* 1 = 0.0277894 loss)
I0318 12:04:37.880333  2460 sgd_solver.cpp:106] Iteration 45000, lr = 0.0025
I0318 12:04:45.888478  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:04:46.885223  2460 solver.cpp:228] Iteration 46000, loss = 0.0182485
I0318 12:04:46.885259  2460 solver.cpp:244]     Train net output #0: loss = 0.0150939 (* 1 = 0.0150939 loss)
I0318 12:04:46.885267  2460 sgd_solver.cpp:106] Iteration 46000, lr = 0.0025
I0318 12:04:55.968832  2460 solver.cpp:228] Iteration 47000, loss = 0.0221171
I0318 12:04:55.968861  2460 solver.cpp:244]     Train net output #0: loss = 0.0181749 (* 1 = 0.0181749 loss)
I0318 12:04:55.968868  2460 sgd_solver.cpp:106] Iteration 47000, lr = 0.0025
I0318 12:05:00.023607  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:05:05.155585  2460 solver.cpp:228] Iteration 48000, loss = 0.0168607
I0318 12:05:05.155612  2460 solver.cpp:244]     Train net output #0: loss = 0.0153877 (* 1 = 0.0153877 loss)
I0318 12:05:05.155616  2460 sgd_solver.cpp:106] Iteration 48000, lr = 0.0025
I0318 12:05:14.102007  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:05:14.210454  2460 solver.cpp:228] Iteration 49000, loss = 0.0205363
I0318 12:05:14.210482  2460 solver.cpp:244]     Train net output #0: loss = 0.01686 (* 1 = 0.01686 loss)
I0318 12:05:14.210487  2460 sgd_solver.cpp:106] Iteration 49000, lr = 0.0025
I0318 12:05:23.244087  2460 solver.cpp:337] Iteration 50000, Testing net (#0)
I0318 12:05:23.244103  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:05:23.244105  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:05:23.244110  2460 net.cpp:709] Ignoring source layer loss
I0318 12:05:23.476323  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9926
I0318 12:05:23.483314  2460 solver.cpp:228] Iteration 50000, loss = 0.0189512
I0318 12:05:23.483332  2460 solver.cpp:244]     Train net output #0: loss = 0.00909992 (* 1 = 0.00909992 loss)
I0318 12:05:23.483338  2460 sgd_solver.cpp:106] Iteration 50000, lr = 0.0025
I0318 12:05:28.564685  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:05:32.642969  2460 solver.cpp:228] Iteration 51000, loss = 0.0204303
I0318 12:05:32.642998  2460 solver.cpp:244]     Train net output #0: loss = 0.00775366 (* 1 = 0.00775366 loss)
I0318 12:05:32.643003  2460 sgd_solver.cpp:106] Iteration 51000, lr = 0.0025
I0318 12:05:41.638923  2460 solver.cpp:228] Iteration 52000, loss = 0.0199317
I0318 12:05:41.638952  2460 solver.cpp:244]     Train net output #0: loss = 0.00809271 (* 1 = 0.00809271 loss)
I0318 12:05:41.638962  2460 sgd_solver.cpp:106] Iteration 52000, lr = 0.0025
I0318 12:05:41.890911  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:05:50.703025  2460 solver.cpp:228] Iteration 53000, loss = 0.017208
I0318 12:05:50.703078  2460 solver.cpp:244]     Train net output #0: loss = 0.0105206 (* 1 = 0.0105206 loss)
I0318 12:05:50.703083  2460 sgd_solver.cpp:106] Iteration 53000, lr = 0.0025
I0318 12:05:55.268848  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:05:59.886086  2460 solver.cpp:228] Iteration 54000, loss = 0.0172028
I0318 12:05:59.886116  2460 solver.cpp:244]     Train net output #0: loss = 0.0121088 (* 1 = 0.0121088 loss)
I0318 12:05:59.886122  2460 sgd_solver.cpp:106] Iteration 54000, lr = 0.0025
I0318 12:06:09.070371  2460 solver.cpp:337] Iteration 55000, Testing net (#0)
I0318 12:06:09.070391  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:06:09.070395  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:06:09.070402  2460 net.cpp:709] Ignoring source layer loss
I0318 12:06:09.201946  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:06:09.305471  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9925
I0318 12:06:09.311511  2460 solver.cpp:228] Iteration 55000, loss = 0.0191027
I0318 12:06:09.311533  2460 solver.cpp:244]     Train net output #0: loss = 0.0259364 (* 1 = 0.0259364 loss)
I0318 12:06:09.311542  2460 sgd_solver.cpp:106] Iteration 55000, lr = 0.0025
I0318 12:06:18.481490  2460 solver.cpp:228] Iteration 56000, loss = 0.0212307
I0318 12:06:18.481519  2460 solver.cpp:244]     Train net output #0: loss = 0.0223781 (* 1 = 0.0223781 loss)
I0318 12:06:18.481524  2460 sgd_solver.cpp:106] Iteration 56000, lr = 0.0025
I0318 12:06:23.697520  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:06:27.651754  2460 solver.cpp:228] Iteration 57000, loss = 0.018085
I0318 12:06:27.651790  2460 solver.cpp:244]     Train net output #0: loss = 0.0359222 (* 1 = 0.0359222 loss)
I0318 12:06:27.651798  2460 sgd_solver.cpp:106] Iteration 57000, lr = 0.0025
I0318 12:06:36.684938  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:06:36.784776  2460 solver.cpp:228] Iteration 58000, loss = 0.01677
I0318 12:06:36.784803  2460 solver.cpp:244]     Train net output #0: loss = 0.0137571 (* 1 = 0.0137571 loss)
I0318 12:06:36.784808  2460 sgd_solver.cpp:106] Iteration 58000, lr = 0.0025
I0318 12:06:45.865912  2460 solver.cpp:228] Iteration 59000, loss = 0.0199906
I0318 12:06:45.865941  2460 solver.cpp:244]     Train net output #0: loss = 0.016743 (* 1 = 0.016743 loss)
I0318 12:06:45.865945  2460 sgd_solver.cpp:106] Iteration 59000, lr = 0.0025
I0318 12:06:50.076972  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:06:55.047546  2460 solver.cpp:337] Iteration 60000, Testing net (#0)
I0318 12:06:55.047581  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:06:55.047585  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:06:55.047590  2460 net.cpp:709] Ignoring source layer loss
I0318 12:06:55.278784  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9923
I0318 12:06:55.284253  2460 solver.cpp:228] Iteration 60000, loss = 0.016706
I0318 12:06:55.284272  2460 solver.cpp:244]     Train net output #0: loss = 0.039747 (* 1 = 0.039747 loss)
I0318 12:06:55.284278  2460 sgd_solver.cpp:106] Iteration 60000, lr = 0.00125
I0318 12:07:02.691018  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:07:04.401499  2460 solver.cpp:228] Iteration 61000, loss = 0.0168432
I0318 12:07:04.401535  2460 solver.cpp:244]     Train net output #0: loss = 0.00951323 (* 1 = 0.00951323 loss)
I0318 12:07:04.401542  2460 sgd_solver.cpp:106] Iteration 61000, lr = 0.00125
I0318 12:07:13.431731  2460 solver.cpp:228] Iteration 62000, loss = 0.0178955
I0318 12:07:13.431761  2460 solver.cpp:244]     Train net output #0: loss = 0.0116059 (* 1 = 0.0116059 loss)
I0318 12:07:13.431766  2460 sgd_solver.cpp:106] Iteration 62000, lr = 0.00125
I0318 12:07:16.932567  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:07:22.428827  2460 solver.cpp:228] Iteration 63000, loss = 0.020199
I0318 12:07:22.428855  2460 solver.cpp:244]     Train net output #0: loss = 0.0219356 (* 1 = 0.0219356 loss)
I0318 12:07:22.428860  2460 sgd_solver.cpp:106] Iteration 63000, lr = 0.00125
I0318 12:07:29.628968  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:07:31.499410  2460 solver.cpp:228] Iteration 64000, loss = 0.0193781
I0318 12:07:31.499439  2460 solver.cpp:244]     Train net output #0: loss = 0.0257374 (* 1 = 0.0257374 loss)
I0318 12:07:31.499445  2460 sgd_solver.cpp:106] Iteration 64000, lr = 0.00125
I0318 12:07:40.736505  2460 solver.cpp:337] Iteration 65000, Testing net (#0)
I0318 12:07:40.736521  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:07:40.736523  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:07:40.736527  2460 net.cpp:709] Ignoring source layer loss
I0318 12:07:40.967994  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9922
I0318 12:07:40.973480  2460 solver.cpp:228] Iteration 65000, loss = 0.0166653
I0318 12:07:40.973498  2460 solver.cpp:244]     Train net output #0: loss = 0.0300644 (* 1 = 0.0300644 loss)
I0318 12:07:40.973505  2460 sgd_solver.cpp:106] Iteration 65000, lr = 0.00125
I0318 12:07:42.376070  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:07:50.058022  2460 solver.cpp:228] Iteration 66000, loss = 0.014766
I0318 12:07:50.058048  2460 solver.cpp:244]     Train net output #0: loss = 0.0189182 (* 1 = 0.0189182 loss)
I0318 12:07:50.058054  2460 sgd_solver.cpp:106] Iteration 66000, lr = 0.00125
I0318 12:07:55.154799  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:07:59.241392  2460 solver.cpp:228] Iteration 67000, loss = 0.017959
I0318 12:07:59.241421  2460 solver.cpp:244]     Train net output #0: loss = 0.00564076 (* 1 = 0.00564076 loss)
I0318 12:07:59.241428  2460 sgd_solver.cpp:106] Iteration 67000, lr = 0.00125
I0318 12:08:08.469414  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:08:08.474906  2460 solver.cpp:228] Iteration 68000, loss = 0.0172363
I0318 12:08:08.474934  2460 solver.cpp:244]     Train net output #0: loss = 0.0129278 (* 1 = 0.0129278 loss)
I0318 12:08:08.474941  2460 sgd_solver.cpp:106] Iteration 68000, lr = 0.00125
I0318 12:08:17.788100  2460 solver.cpp:228] Iteration 69000, loss = 0.0193314
I0318 12:08:17.788127  2460 solver.cpp:244]     Train net output #0: loss = 0.0128221 (* 1 = 0.0128221 loss)
I0318 12:08:17.788132  2460 sgd_solver.cpp:106] Iteration 69000, lr = 0.00125
I0318 12:08:21.464654  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:08:27.120004  2460 solver.cpp:337] Iteration 70000, Testing net (#0)
I0318 12:08:27.120020  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:08:27.120023  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:08:27.120028  2460 net.cpp:709] Ignoring source layer loss
I0318 12:08:27.351697  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9927
I0318 12:08:27.357072  2460 solver.cpp:228] Iteration 70000, loss = 0.0156559
I0318 12:08:27.357091  2460 solver.cpp:244]     Train net output #0: loss = 0.0244833 (* 1 = 0.0244833 loss)
I0318 12:08:27.357097  2460 sgd_solver.cpp:106] Iteration 70000, lr = 0.00125
I0318 12:08:33.964154  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:08:36.484227  2460 solver.cpp:228] Iteration 71000, loss = 0.0174873
I0318 12:08:36.484254  2460 solver.cpp:244]     Train net output #0: loss = 0.014502 (* 1 = 0.014502 loss)
I0318 12:08:36.484259  2460 sgd_solver.cpp:106] Iteration 71000, lr = 0.00125
I0318 12:08:45.511509  2460 solver.cpp:228] Iteration 72000, loss = 0.0175076
I0318 12:08:45.511595  2460 solver.cpp:244]     Train net output #0: loss = 0.0137307 (* 1 = 0.0137307 loss)
I0318 12:08:45.511600  2460 sgd_solver.cpp:106] Iteration 72000, lr = 0.00125
I0318 12:08:46.893962  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:08:54.662590  2460 solver.cpp:228] Iteration 73000, loss = 0.0186069
I0318 12:08:54.662616  2460 solver.cpp:244]     Train net output #0: loss = 0.0200228 (* 1 = 0.0200228 loss)
I0318 12:08:54.662621  2460 sgd_solver.cpp:106] Iteration 73000, lr = 0.00125
I0318 12:09:03.814366  2460 solver.cpp:228] Iteration 74000, loss = 0.0178496
I0318 12:09:03.814393  2460 solver.cpp:244]     Train net output #0: loss = 0.023968 (* 1 = 0.023968 loss)
I0318 12:09:03.814399  2460 sgd_solver.cpp:106] Iteration 74000, lr = 0.00125
I0318 12:09:03.827095  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:09:13.152820  2460 solver.cpp:337] Iteration 75000, Testing net (#0)
I0318 12:09:13.152837  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:09:13.152839  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:09:13.152844  2460 net.cpp:709] Ignoring source layer loss
I0318 12:09:13.384721  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9923
I0318 12:09:13.391731  2460 solver.cpp:228] Iteration 75000, loss = 0.0167583
I0318 12:09:13.391751  2460 solver.cpp:244]     Train net output #0: loss = 0.0168801 (* 1 = 0.0168801 loss)
I0318 12:09:13.391755  2460 sgd_solver.cpp:106] Iteration 75000, lr = 0.00125
I0318 12:09:15.741148  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:09:22.482311  2460 solver.cpp:228] Iteration 76000, loss = 0.0174647
I0318 12:09:22.482338  2460 solver.cpp:244]     Train net output #0: loss = 0.00988543 (* 1 = 0.00988543 loss)
I0318 12:09:22.482343  2460 sgd_solver.cpp:106] Iteration 76000, lr = 0.00125
I0318 12:09:29.238845  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:09:31.533624  2460 solver.cpp:228] Iteration 77000, loss = 0.0173156
I0318 12:09:31.533653  2460 solver.cpp:244]     Train net output #0: loss = 0.029074 (* 1 = 0.029074 loss)
I0318 12:09:31.533658  2460 sgd_solver.cpp:106] Iteration 77000, lr = 0.00125
I0318 12:09:40.622344  2460 solver.cpp:228] Iteration 78000, loss = 0.0186655
I0318 12:09:40.622372  2460 solver.cpp:244]     Train net output #0: loss = 0.0248234 (* 1 = 0.0248234 loss)
I0318 12:09:40.622377  2460 sgd_solver.cpp:106] Iteration 78000, lr = 0.00125
I0318 12:09:42.198869  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:09:49.668578  2460 solver.cpp:228] Iteration 79000, loss = 0.0166506
I0318 12:09:49.668639  2460 solver.cpp:244]     Train net output #0: loss = 0.0114986 (* 1 = 0.0114986 loss)
I0318 12:09:49.668648  2460 sgd_solver.cpp:106] Iteration 79000, lr = 0.00125
I0318 12:09:55.005089  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:09:59.071593  2460 solver.cpp:337] Iteration 80000, Testing net (#0)
I0318 12:09:59.071609  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:09:59.071612  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:09:59.071619  2460 net.cpp:709] Ignoring source layer loss
I0318 12:09:59.368026  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9925
I0318 12:09:59.373548  2460 solver.cpp:228] Iteration 80000, loss = 0.0179881
I0318 12:09:59.373569  2460 solver.cpp:244]     Train net output #0: loss = 0.0130708 (* 1 = 0.0130708 loss)
I0318 12:09:59.373579  2460 sgd_solver.cpp:106] Iteration 80000, lr = 0.000625
I0318 12:10:08.525457  2460 solver.cpp:228] Iteration 81000, loss = 0.0147433
I0318 12:10:08.525486  2460 solver.cpp:244]     Train net output #0: loss = 0.0201622 (* 1 = 0.0201622 loss)
I0318 12:10:08.525492  2460 sgd_solver.cpp:106] Iteration 81000, lr = 0.000625
I0318 12:10:10.268131  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:10:17.583286  2460 solver.cpp:228] Iteration 82000, loss = 0.0187073
I0318 12:10:17.583317  2460 solver.cpp:244]     Train net output #0: loss = 0.0258972 (* 1 = 0.0258972 loss)
I0318 12:10:17.583323  2460 sgd_solver.cpp:106] Iteration 82000, lr = 0.000625
I0318 12:10:24.967520  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:10:26.782269  2460 solver.cpp:228] Iteration 83000, loss = 0.0177036
I0318 12:10:26.782304  2460 solver.cpp:244]     Train net output #0: loss = 0.0143806 (* 1 = 0.0143806 loss)
I0318 12:10:26.782312  2460 sgd_solver.cpp:106] Iteration 83000, lr = 0.000625
I0318 12:10:35.933135  2460 solver.cpp:228] Iteration 84000, loss = 0.0179952
I0318 12:10:35.933161  2460 solver.cpp:244]     Train net output #0: loss = 0.0137086 (* 1 = 0.0137086 loss)
I0318 12:10:35.933166  2460 sgd_solver.cpp:106] Iteration 84000, lr = 0.000625
I0318 12:10:38.402130  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:10:44.987113  2460 solver.cpp:337] Iteration 85000, Testing net (#0)
I0318 12:10:44.987129  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:10:44.987131  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:10:44.987136  2460 net.cpp:709] Ignoring source layer loss
I0318 12:10:45.277534  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9927
I0318 12:10:45.283021  2460 solver.cpp:228] Iteration 85000, loss = 0.0176374
I0318 12:10:45.283041  2460 solver.cpp:244]     Train net output #0: loss = 0.0067127 (* 1 = 0.0067127 loss)
I0318 12:10:45.283046  2460 sgd_solver.cpp:106] Iteration 85000, lr = 0.000625
I0318 12:10:51.945111  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:10:54.400449  2460 solver.cpp:228] Iteration 86000, loss = 0.0142534
I0318 12:10:54.400477  2460 solver.cpp:244]     Train net output #0: loss = 0.0135651 (* 1 = 0.0135651 loss)
I0318 12:10:54.400483  2460 sgd_solver.cpp:106] Iteration 86000, lr = 0.000625
I0318 12:11:03.440897  2460 solver.cpp:228] Iteration 87000, loss = 0.0163353
I0318 12:11:03.440990  2460 solver.cpp:244]     Train net output #0: loss = 0.0161361 (* 1 = 0.0161361 loss)
I0318 12:11:03.440999  2460 sgd_solver.cpp:106] Iteration 87000, lr = 0.000625
I0318 12:11:05.246176  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:11:12.582733  2460 solver.cpp:228] Iteration 88000, loss = 0.0176981
I0318 12:11:12.582762  2460 solver.cpp:244]     Train net output #0: loss = 0.0142034 (* 1 = 0.0142034 loss)
I0318 12:11:12.582767  2460 sgd_solver.cpp:106] Iteration 88000, lr = 0.000625
I0318 12:11:20.681995  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:11:21.626210  2460 solver.cpp:228] Iteration 89000, loss = 0.0209229
I0318 12:11:21.626240  2460 solver.cpp:244]     Train net output #0: loss = 0.015224 (* 1 = 0.015224 loss)
I0318 12:11:21.626245  2460 sgd_solver.cpp:106] Iteration 89000, lr = 0.000625
I0318 12:11:30.767313  2460 solver.cpp:337] Iteration 90000, Testing net (#0)
I0318 12:11:30.767328  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:11:30.767330  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:11:30.767335  2460 net.cpp:709] Ignoring source layer loss
I0318 12:11:30.998672  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9925
I0318 12:11:31.006237  2460 solver.cpp:228] Iteration 90000, loss = 0.0167688
I0318 12:11:31.006256  2460 solver.cpp:244]     Train net output #0: loss = 0.00842981 (* 1 = 0.00842981 loss)
I0318 12:11:31.006263  2460 sgd_solver.cpp:106] Iteration 90000, lr = 0.000625
I0318 12:11:33.259259  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:11:40.042310  2460 solver.cpp:228] Iteration 91000, loss = 0.0191985
I0318 12:11:40.042393  2460 solver.cpp:244]     Train net output #0: loss = 0.0144605 (* 1 = 0.0144605 loss)
I0318 12:11:40.042399  2460 sgd_solver.cpp:106] Iteration 91000, lr = 0.000625
I0318 12:11:48.230289  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:11:49.105875  2460 solver.cpp:228] Iteration 92000, loss = 0.0152766
I0318 12:11:49.105903  2460 solver.cpp:244]     Train net output #0: loss = 0.02208 (* 1 = 0.02208 loss)
I0318 12:11:49.105908  2460 sgd_solver.cpp:106] Iteration 92000, lr = 0.000625
I0318 12:11:58.176934  2460 solver.cpp:228] Iteration 93000, loss = 0.0174878
I0318 12:11:58.176962  2460 solver.cpp:244]     Train net output #0: loss = 0.0114819 (* 1 = 0.0114819 loss)
I0318 12:11:58.176969  2460 sgd_solver.cpp:106] Iteration 93000, lr = 0.000625
I0318 12:12:01.653977  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:12:07.301928  2460 solver.cpp:228] Iteration 94000, loss = 0.0157898
I0318 12:12:07.301962  2460 solver.cpp:244]     Train net output #0: loss = 0.0201091 (* 1 = 0.0201091 loss)
I0318 12:12:07.301969  2460 sgd_solver.cpp:106] Iteration 94000, lr = 0.000625
I0318 12:12:15.932862  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:12:16.353134  2460 solver.cpp:337] Iteration 95000, Testing net (#0)
I0318 12:12:16.353152  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:12:16.353153  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:12:16.353158  2460 net.cpp:709] Ignoring source layer loss
I0318 12:12:16.595774  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9927
I0318 12:12:16.601161  2460 solver.cpp:228] Iteration 95000, loss = 0.0174004
I0318 12:12:16.601178  2460 solver.cpp:244]     Train net output #0: loss = 0.0173755 (* 1 = 0.0173755 loss)
I0318 12:12:16.601184  2460 sgd_solver.cpp:106] Iteration 95000, lr = 0.000625
I0318 12:12:25.551002  2460 solver.cpp:228] Iteration 96000, loss = 0.0179131
I0318 12:12:25.551028  2460 solver.cpp:244]     Train net output #0: loss = 0.0273218 (* 1 = 0.0273218 loss)
I0318 12:12:25.551033  2460 sgd_solver.cpp:106] Iteration 96000, lr = 0.000625
I0318 12:12:34.593039  2460 solver.cpp:228] Iteration 97000, loss = 0.018328
I0318 12:12:34.593073  2460 solver.cpp:244]     Train net output #0: loss = 0.00635479 (* 1 = 0.00635479 loss)
I0318 12:12:34.593080  2460 sgd_solver.cpp:106] Iteration 97000, lr = 0.000625
I0318 12:12:34.648921  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:12:43.702003  2460 solver.cpp:228] Iteration 98000, loss = 0.0162812
I0318 12:12:43.702031  2460 solver.cpp:244]     Train net output #0: loss = 0.0173101 (* 1 = 0.0173101 loss)
I0318 12:12:43.702035  2460 sgd_solver.cpp:106] Iteration 98000, lr = 0.000625
I0318 12:12:50.086391  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:12:52.704314  2460 solver.cpp:228] Iteration 99000, loss = 0.0181326
I0318 12:12:52.704349  2460 solver.cpp:244]     Train net output #0: loss = 0.0116747 (* 1 = 0.0116747 loss)
I0318 12:12:52.704355  2460 sgd_solver.cpp:106] Iteration 99000, lr = 0.000625
I0318 12:13:01.764248  2460 solver.cpp:454] Snapshotting to binary proto file mnist_iter_100000.caffemodel
I0318 12:13:01.766556  2460 sgd_solver.cpp:273] Snapshotting solver state to binary proto file mnist_iter_100000.solverstate
I0318 12:13:01.774196  2460 solver.cpp:317] Iteration 100000, loss = 0.0151846
I0318 12:13:01.774214  2460 solver.cpp:337] Iteration 100000, Testing net (#0)
I0318 12:13:01.774219  2460 net.cpp:709] Ignoring source layer data_drop
I0318 12:13:01.774220  2460 net.cpp:709] Ignoring source layer data_vision
I0318 12:13:01.774224  2460 net.cpp:709] Ignoring source layer loss
I0318 12:13:01.994133  2460 blocking_queue.cpp:50] Data layer prefetch queue empty
I0318 12:13:02.006359  2460 solver.cpp:404]     Test net output #0: accuracy = 0.9926
I0318 12:13:02.006376  2460 solver.cpp:322] Optimization Done.
I0318 12:13:02.006379  2460 caffe.cpp:254] Optimization Done.
